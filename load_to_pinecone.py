import os
import json
from dotenv import load_dotenv
from langchain_openai import OpenAIEmbeddings
from langchain_pinecone import PineconeVectorStore as LangchainPinecone
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from pinecone import Pinecone as pc_client, ServerlessSpec
from tqdm import tqdm

# üì• Cargar variables de entorno
load_dotenv()

# üîê API keys y configuraci√≥n
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
PINECONE_ENVIRONMENT = os.getenv("PINECONE_ENVIRONMENT")
PINECONE_INDEX = os.getenv("PINECONE_INDEX")
PINECONE_NAMESPACE = os.getenv("PINECONE_NAMESPACE", "default")

# üîß Configuraciones din√°micas
CHUNK_SIZE = int(os.getenv("CHUNK_SIZE", 1000))
CHUNK_OVERLAP = int(os.getenv("CHUNK_OVERLAP", 100))
BATCH_SIZE = int(os.getenv("BATCH_SIZE", 200))

# ‚úÖ Validar configuraci√≥n base
if not all([OPENAI_API_KEY, PINECONE_API_KEY, PINECONE_ENVIRONMENT, PINECONE_INDEX]):
    raise ValueError("‚ùå Faltan variables cr√≠ticas en el .env")

# üß† Inicializar modelo de embeddings
embedding_model = OpenAIEmbeddings(
    model="text-embedding-3-small",
    openai_api_key=OPENAI_API_KEY
)

# üå≤ Cliente Pinecone
pc = pc_client(api_key=PINECONE_API_KEY)

# Crear √≠ndice si no existe
if PINECONE_INDEX not in pc.list_indexes().names():
    pc.create_index(
        name=PINECONE_INDEX,
        dimension=1536,
        metric="cosine",
        spec=ServerlessSpec(cloud="aws", region=PINECONE_ENVIRONMENT)
    )

index = pc.Index(PINECONE_INDEX)

# üìÑ Leer los documentos
with open("output.json", "r", encoding="utf-8") as f:
    raw_data = json.load(f)

docs = [
    Document(page_content=entry["text"], metadata={"source": entry["url"]})
    for entry in raw_data if "text" in entry and entry["text"].strip()
]

print(f"üìÑ {len(docs)} documentos cargados.")

# ‚úÇÔ∏è Dividir los documentos
splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)
docs_split = splitter.split_documents(docs)

print(f"üîπ {len(docs_split)} fragmentos generados.")

# üì¶ Subida en lotes
vectorstore = LangchainPinecone(
    embedding=embedding_model,
    index_name=PINECONE_INDEX,
    namespace=PINECONE_NAMESPACE
)

for i in tqdm(range(0, len(docs_split), BATCH_SIZE), desc="üì§ Subiendo a Pinecone"):
    batch = docs_split[i:i + BATCH_SIZE]
    vectorstore.add_documents(batch)

print("‚úÖ Embeddings subidos exitosamente por lotes.")
